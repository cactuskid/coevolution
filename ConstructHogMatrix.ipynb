{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "import pandas as pd\n",
    "omadatalocation = '/data/databases/OMA/OMA.2.1.1/data/OmaServer.h5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "omadata = tb.open_file(omadatalocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description([('Fam', '()i4'), ('ID', '()S255'), ('Level', '()S255')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tables/leaf.py:396: PerformanceWarning: The Leaf ``/_i_HogLevel/ID/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(omadata.root.HogLevel.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(299330, 'HOG:0299330', 'Chromadorea')\n",
      " (299330, 'HOG:0299330', 'Pristionchus pacificus')\n",
      " (299330, 'HOG:0299330', 'Onchocercidae')\n",
      " (299330, 'HOG:0299330', 'Brugia malayi')\n",
      " (299330, 'HOG:0299330', 'Loa loa')\n",
      " (299330, 'HOG:0299330', 'Onchocerca volvulus')\n",
      " (299330, 'HOG:0299330', 'Rhabditida')\n",
      " (299330, 'HOG:0299330', 'Strongyloides ratti')\n",
      " (299330, 'HOG:0299330', 'Caenorhabditis')\n",
      " (299330, 'HOG:0299330', 'Caenorhabditis brenneri')]\n"
     ]
    }
   ],
   "source": [
    "print(omadata.root.HogLevel[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using sets to count the unique occurences in each column</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920\n",
      "taxa\n",
      "2715332\n",
      "Hogs\n",
      "589223\n",
      "Fams\n",
      "17467463\n",
      "entries\n"
     ]
    }
   ],
   "source": [
    "Taxa = set([])\n",
    "Hogs = set([])\n",
    "Fams = set([])\n",
    "rows = 0\n",
    "\n",
    "for row in omadata.root.HogLevel:\n",
    "    Taxa.add(row[2])\n",
    "    Hogs.add(row[1])\n",
    "    Fams.add(row[0])\n",
    "    rows+=1\n",
    "\n",
    "print(len(Taxa))\n",
    "print('taxa')\n",
    "print(len(Hogs))\n",
    "print('Hogs')\n",
    "print(len(Fams))\n",
    "print('Fams')\n",
    "print(rows)\n",
    "print('entries')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "handle1 = open('./hogs.pkl','w')\n",
    "handle2= open('./fams.pkl','w')\n",
    "handle3 = open('./taxa.pkl','w')\n",
    "\n",
    "pickle.dump(Hogs,handle1,-1)\n",
    "pickle.dump(Fams,handle2,-1)\n",
    "pickle.dump(Taxa,handle3,-1)\n",
    "\n",
    "handle1.close()\n",
    "handle2.close()\n",
    "handle3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create a dictionary for HOGs and families </h1> The taxonomy dict isn't really necesary since we have one from the other taxonomy notebook that will be more useful in selecting our reference taxa later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the column dictionary from the taxonomic analysis here\n",
    "import pickle\n",
    "import numpy as np\n",
    "handle1 = open('./column_dict.pkl', 'r')\n",
    "handle2= open('./NameToID.pkl','r')\n",
    "\n",
    "NameToID = pickle.load(handle2, )\n",
    "column_dict = pickle.load(handle1)\n",
    "handle1.close()\n",
    "handle2.close()\n",
    "\n",
    "\n",
    "handle1 = open('./hogs.pkl','r')\n",
    "handle2= open('./fams.pkl','r')\n",
    "handle3 = open('./taxa.pkl','r')\n",
    "\n",
    "Hogs = pickle.load(handle1)\n",
    "Fams = pickle.load(handle2)\n",
    "Taxa = pickle.load(handle3)\n",
    "\n",
    "handle1.close()\n",
    "handle2.close()\n",
    "handle3.close()\n",
    "\n",
    "\n",
    "coldict={}\n",
    "coldictReverse = {}\n",
    "for i,taxon in enumerate(Taxa):\n",
    "    coldict[taxon]=i\n",
    "    coldictReverse[i] = taxon\n",
    "rowdict={}\n",
    "rowdictReverse={}\n",
    "#define a row for each unique hog\n",
    "for i,hog in enumerate(Hogs):\n",
    "    rowdict[hog]= i\n",
    "    rowdictReverse[i]=hog\n",
    "rowdictFam ={}\n",
    "rowdictFamReverse = {}\n",
    "#define a row for each unique family\n",
    "#dont know if we'll use this...\n",
    "for i,fam in enumerate(Fams):\n",
    "    rowdictFam[fam] = i\n",
    "    rowdictFamReverse[i]=fam\n",
    "\n",
    "\n",
    "\n",
    "colmax=max(column_dict.values())+1\n",
    "rowmax=len(Hogs)+1\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def retMatrixChunk( Hogdatachunk ):\n",
    "    matrix = lil_matrix((rowmax,colmax))\n",
    "    fams,hogs,taxa =Hogdatachunk\n",
    "    matrix[ ([rowdict[hog] for hog in hogs] , [column_dict[NameToID [taxon]] for taxon in taxa] ) ] = 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def retMatrixindex( Hogdatachunk ):\n",
    "    fams,hogs,taxa =Hogdatachunk\n",
    "    return ([rowdict[hog] for hog in hogs] , [column_dict[NameToID [taxon]] for taxon in taxa] ) \n",
    "\n",
    "def returnDataChunk( HogData , chunksize):\n",
    "    i=0\n",
    "    chunk =[]\n",
    "    while len(chunk) > 0 or i == 0:\n",
    "        chunk = HogData[i*chunksize:(i+1)*chunksize]\n",
    "        fams,hogs,taxa = zip(*chunk)\n",
    "        i +=1\n",
    "        yield [fams, hogs, taxa]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just testing out the generator and the transformation to sparse matrix format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00739299999999\n",
      "0.000868000000011\n",
      "0.000579000000002\n",
      "0.000332\n",
      "0.000570999999994\n",
      "0.000325000000004\n",
      "0.000602999999998\n",
      "0.000478999999999\n",
      "0.000529999999998\n",
      "0.000480999999994\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "it = returnDataChunk(omadata.root.HogLevel,100)\n",
    "for x in range(10):\n",
    "    start = time.clock()\n",
    "\n",
    "    retMatrixindex(next(it))\n",
    "\n",
    "    print(str( time.clock()-start ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Transform taxonomic levels in OMA to sparse matrix</h1>\n",
    "To avoid passing vars around we'll use queues to pass chunks of OMA and return chunks of the final matrix.\n",
    "\n",
    "Each taxonomic level corresponds to a column and each row corresponds to a HOG. This takes an hour or two to calculate for all Hogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#return datachunks to MP function\n",
    "chunksize = 10000\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "import time\n",
    "import gc\n",
    "import sys \n",
    "\n",
    "\n",
    "datagenerator = returnDataChunk(omadata.root.HogLevel, chunksize ) \n",
    "\n",
    "def worker(q,retq,l):\n",
    "    while True:\n",
    "        data = q.get()\n",
    "        if data == 'DONE':\n",
    "            break\n",
    "        indices = retMatrixindex(data)\n",
    "        retq.put(indices)\n",
    "        del data\n",
    "        gc.collect()  \n",
    "def updater(q,retq,l):\n",
    "    cooevolution_matrix = lil_matrix( (rowmax,colmax) )\n",
    "    while True:\n",
    "        indices = retq.get()\n",
    "        if indices == 'DONE':\n",
    "            break\n",
    "        cooevolution_matrix[indices] = 1\n",
    "        del indices\n",
    "        gc.collect()\n",
    "    q.put(cooevolution_matrix)\n",
    "\n",
    "l = mp.Lock()\n",
    "cores = mp.cpu_count()\n",
    "q = mp.Queue(maxsize = int(cores/1.5))\n",
    "retq = mp.Queue(maxsize = int(cores/1.5))\n",
    "\n",
    "processes =[]\n",
    "\n",
    "for i in range(cores):\n",
    "    t = mp.Process(target=worker, args=(q,retq,l)  ) \n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "    processes.append(t)\n",
    "\n",
    "u = mp.Process(target=updater, args=(q,retq,l)  ) \n",
    "u.daemon = True\n",
    "u.start()\n",
    "\n",
    "count =0\n",
    "start = 0\n",
    "stop = chunksize\n",
    "data = zip(*omadata.root.HogLevel[start:stop])\n",
    "q.put(data)\n",
    "\n",
    "start = time.clock\n",
    "while len(data)>0:\n",
    "    time.sleep(1)\n",
    "    while q.full() == False:\n",
    "        count += 1\n",
    "        start = stop\n",
    "        stop+=chunksize\n",
    "        data = zip(*omadata.root.HogLevel[start:stop])\n",
    "        if len(data) == 0:\n",
    "            for p in range(len(processes)):\n",
    "                q.put('DONE')\n",
    "            break\n",
    "        else:\n",
    "            q.put(data)\n",
    "\n",
    "            \n",
    "retq.put('DONE')\n",
    "cooevolution_matrix = q.get()        \n",
    "print 'DONE!!!!!'\n",
    "#save the matrix\n",
    "stop = time.clock()\n",
    "print 'hours'\n",
    "print (stop - start) / 3600\n",
    "handle1=open('./BigCoEvMatrix.pkl' , 'w')\n",
    "pickle.dump(cooevolution_matrix,handle1, -1)\n",
    "handle1.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from scipy.sparse import find\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "handle1 =open('./BigCoEvMatrix.pkl' , 'r')\n",
    "cooevolution_matrix = pickle.load(handle1)\n",
    "handle1.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, LeanMinHash , MinHashLSHForest, HyperLogLogPlusPlus\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def returnHashes(data, verbose= False):\n",
    "    rowstart, rowend , dataChunk = data\n",
    "    indices = np.vstack( find( dataChunk )).astype(np.int).T\n",
    "    if verbose == True:\n",
    "        print indices \n",
    "    hashes ={}\n",
    "    #update hashes with presence in species\n",
    "    for row in range(indices.shape[0]):\n",
    "        hashes[rowstart+indices[row,0]] = MinHash()\n",
    "        hashes[rowstart+indices[row,0]].update( indices[row,1] )\n",
    "    #turn to small hashes\n",
    "    for row in hashes.keys():\n",
    "        hashes[row]=LeanMinHash(hashes[row])\n",
    "    del dataChunk\n",
    "    gc.collect()\n",
    "    return hashes\n",
    "\n",
    "def chopMatrix(bigMatrix,chunksize):\n",
    "    i =0\n",
    "    leftovers = bigMatrix.shape[0]%chunksize\n",
    "    \n",
    "    while i <  bigMatrix.shape[0] - leftovers :\n",
    "        chunk = bigMatrix[i:i+chunksize,:]\n",
    "        yield [i,i+chunksize,chunk]\n",
    "        i+=chunksize\n",
    "    yield [i,i+leftovers,bigMatrix[i:i+leftovers,:]]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> test out the generator and Hashing functions </h1>\n",
    "The generator chops a matrix into blocks of Nrows. The hashing function assigns hash to each HOG rownumber and updates it based on which columns are positive. When the updates are finished the hash is turned to a LeanMinHash with a smaller memory footprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 874    0    1]\n",
      " [ 997    0    1]\n",
      " [1546    0    1]\n",
      " ..., \n",
      " [9506 4166    1]\n",
      " [9508 4166    1]\n",
      " [9509 4166    1]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2d2b4d97ec3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchopMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcooevolution_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturnHashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-aaa9d3db57b8>\u001b[0m in \u001b[0;36mreturnHashes\u001b[0;34m(data, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mhashes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeanMinHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdataChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "data = next(chopMatrix(cooevolution_matrix,10000))\n",
    "start = time.clock()\n",
    "print len(returnHashes(data, verbose= True))\n",
    "print time.clock()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>  calculate a hash for each HOG</h1> the same multiprocessing strategy as before expcept an updater process has the LSFforest and adds the hashes as they are calculated instead of using the main loop in the notebook. The end result is a dictionary of Hashes for each Hog and an LSF forest that includes all the Hashes and is searchable with a hash.\n",
    "\n",
    "\n",
    "The top N neighbors for each HOG can be used as a threshold to avoid calculating other more costly distance metrics for all the HOGs vs all the HOGs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 61865 stored elements in LInked List format>]\n",
      "[10000, 20000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 62495 stored elements in LInked List format>]\n",
      "[20000, 30000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 66082 stored elements in LInked List format>]\n",
      "[30000, 40000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 62334 stored elements in LInked List format>]\n",
      "[40000, 50000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 62752 stored elements in LInked List format>]\n",
      "[50000, 60000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 67482 stored elements in LInked List format>]\n",
      "[60000, 70000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 68744 stored elements in LInked List format>]\n",
      "[70000, 80000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 59787 stored elements in LInked List format>]\n",
      "[80000, 90000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 61923 stored elements in LInked List format>]\n",
      "[90000, 100000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 66038 stored elements in LInked List format>]\n",
      "[100000, 110000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 61580 stored elements in LInked List format>]\n",
      "[110000, 120000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 61571 stored elements in LInked List format>]\n",
      "[120000, 130000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 63983 stored elements in LInked List format>]\n",
      "[130000, 140000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 66070 stored elements in LInked List format>]\n",
      "[140000, 150000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 63443 stored elements in LInked List format>]\n",
      "[150000, 160000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 59855 stored elements in LInked List format>]\n",
      "[160000, 170000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 60422 stored elements in LInked List format>]\n",
      "[170000, 180000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 62475 stored elements in LInked List format>]\n",
      "[180000, 190000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 67495 stored elements in LInked List format>]\n",
      "[190000, 200000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 65831 stored elements in LInked List format>]\n",
      "[200000, 210000, <10000x4167 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 67404 stored elements in LInked List format>]\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, LeanMinHash , MinHashLSHForest, HyperLogLogPlusPlus\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "#use min hashing to generate a hash for each HOG and an LSH forest with all hashes\n",
    "#use the jackard distance with LSH forest to grab top N hits for each HOG-> use as threshhold for distance matrices\n",
    "\n",
    "def worker(q,retque,l):\n",
    "    while True:\n",
    "        time.sleep(.01)\n",
    "        data = q.get()\n",
    "        if data == 'DONE':\n",
    "            break\n",
    "        obj = returnHashes(data)\n",
    "        print 'done hashing'\n",
    "        retq.put(obj)\n",
    "        del obj\n",
    "        del data\n",
    "        gc.collect()\n",
    "       \n",
    "def updater(q,retq,l):\n",
    "    bigdict ={}\n",
    "    forest = MinHashLSHForest(num_perm=128, l=8)\n",
    "    hashesPath = './HOGhashdict'\n",
    "    i=0\n",
    "    while True:\n",
    "        time.sleep(.01)\n",
    "        data = retq.get()\n",
    "        if data == 'DONE':\n",
    "            break\n",
    "        bigdict.update(data)\n",
    "        if len(bigdict)> 100000:\n",
    "            with open(hashesPath + str(i)+'.pkl' , 'w') as pickledump:\n",
    "                pickle.dump(bigdict,pickledump,-1)\n",
    "            bigdict = {}\n",
    "            i +=1\n",
    "        for key in data:\n",
    "            forest.add(key,data[key])\n",
    "        forest.index()\n",
    "        \n",
    "        print 'update done'\n",
    "        del data\n",
    "        \n",
    "        #periodically save the big has dictionary\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "    #pass the final result back\n",
    "    q.put((forest) )\n",
    "\n",
    "\n",
    "chunksize = 10000\n",
    "l = mp.Lock()\n",
    "cores = mp.cpu_count()\n",
    "q = mp.Queue(maxsize = cores/4)\n",
    "retq = mp.Queue(maxsize = cores/4)\n",
    "hpp = HyperLogLogPlusPlus()\n",
    "iterator = chopMatrix(cooevolution_matrix,chunksize)\n",
    "\n",
    "processes =[]\n",
    "for i in range(cores):\n",
    "    t = mp.Process(target=worker, args=(q,retq,l)  ) \n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "    processes.append(t)\n",
    "\n",
    "u = mp.Process(target=updater, args=(q,retq,l)  ) \n",
    "u.daemon = True\n",
    "u.start()\n",
    "count =0\n",
    "while q.empty()==False or count == 0:\n",
    "    time.sleep(.1)\n",
    "    while True:\n",
    "        count += 1\n",
    "        try:\n",
    "            data = next(iterator)\n",
    "            print data\n",
    "            q.put(data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "for p in processes:\n",
    "    q.put('DONE')\n",
    "retq.put('DONE')\n",
    "\n",
    "forest = q.get()\n",
    "#construct the fores here ? \n",
    "q.close()\n",
    "retq.close()\n",
    "del iterator\n",
    "del q\n",
    "del retq\n",
    "gc.collect()\n",
    "\n",
    "#save forest and dictionary.\n",
    "forest.index()\n",
    "handle1 = open('bigforest', 'w' )\n",
    "pickle.dump(forest , handle1, -1)\n",
    "handle1.close()\n",
    "\n",
    "print 'DONE!!!!!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the hamming distance matrix between all hogs using the full matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#calculate the prob of 0 or the prob of 1 for each row\n",
    "def prob_x_y_forBinary_M(two_rows):\n",
    "    px_one = np.sum(two_rows[0,:])/two_rows.shape[1]\n",
    "    px_zero = 1-px_one\n",
    "    py_one = np.sum(two_rows[1,:])/two_rows.shape[1]\n",
    "    py_zero = 1-py_one\n",
    "    return np.array( [px_zero, px_one]), np.array([py_zero, py_one])\n",
    "\n",
    "\n",
    "\n",
    "def pxy_coocurrence_between_two_M(m):\n",
    "    #4 cases for two binary vectors. (x=1,y=1), (x=0,y=1), (x=1,y=0), (x=0,y=0) \n",
    "    #use matrix multiplication to get cooccurences between rows for all cases\n",
    "    pxy_case1 = tf.multiply(m,m.T)/m.shape[1]\n",
    "    pxy_case2 = tf.multiply(1-m, m.T )/m.shape[1]\n",
    "    pxy_case3 = tf.multiply(m, 1-m.T)/m.shape[1]\n",
    "    pxy_case4 =1 -tf.add([pxy_case1, pxy_case2 , pxy_case3])\n",
    "    #Ncase 4 is whatever is leftover\n",
    "    return pxy_case1, pxy_case2,pxy_case3, pxy_case4\n",
    "\n",
    "def MI(probx,proby, cooccurenceMat ):\n",
    "    #calculate MI of two rows\n",
    "    px,py = prob_x_y(two_rows)\n",
    "    pxy = pxy_coocurrence_between_two_rows(two_rows)\n",
    "    MI=0\n",
    "    for x in [0,1]:\n",
    "        for y in [0,1]:\n",
    "                MI+= pxy[x,y]*np.log( pxy[x,y] / (px[x] * py[y]) ) \n",
    "    return MI\n",
    "\n",
    "def Hamming_dist(cooccurenceMats):\n",
    "    return coocurrenceMats[1]*coocurrenceMats[1].shape[1]+cooccurenceMats[2]*coocurrenceMats[1].shape[1]\n",
    "\n",
    "def jaccard_dist(two_rows):\n",
    "    pxy = pxy_coocurrence_between_two_rows(two_rows)\n",
    "    return (pxy[0,0]+pxy[1,1] - pxy[0,1]+pxy[1,0]) / pxy[0,0]+pxy[1,1] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge hogs -> merge hashes\n",
    "\n",
    "def mergeHogs()\n",
    "    \n",
    "results = pool.map( )    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run PCA to filter out overrepresented species and project reduced matrix down to species subspace\n",
    "\n",
    "\n",
    "\n",
    "#calculate distance metrics in species subpspace\n",
    "\n",
    "    #hamming\n",
    "    #euclidian\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
